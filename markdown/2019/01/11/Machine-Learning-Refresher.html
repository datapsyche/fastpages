<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Data Mining Map by Saed Sayad | False Positive</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Data Mining Map by Saed Sayad" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Data Mining Map by Saed Sayad." />
<meta property="og:description" content="Data Mining Map by Saed Sayad." />
<link rel="canonical" href="https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html" />
<meta property="og:url" content="https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html" />
<meta property="og:site_name" content="False Positive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-11T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2019-01-11T00:00:00-06:00","headline":"Data Mining Map by Saed Sayad","description":"Data Mining Map by Saed Sayad.","mainEntityOfPage":{"@type":"WebPage","@id":"https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html"},"@type":"BlogPosting","url":"https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html","dateModified":"2019-01-11T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastpages/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://datapsyche.github.io/fastpages/feed.xml" title="False Positive" /><link rel="shortcut icon" type="image/x-icon" href="/fastpages/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Data Mining Map by Saed Sayad | False Positive</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Data Mining Map by Saed Sayad" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Data Mining Map by Saed Sayad." />
<meta property="og:description" content="Data Mining Map by Saed Sayad." />
<link rel="canonical" href="https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html" />
<meta property="og:url" content="https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html" />
<meta property="og:site_name" content="False Positive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-11T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2019-01-11T00:00:00-06:00","headline":"Data Mining Map by Saed Sayad","description":"Data Mining Map by Saed Sayad.","mainEntityOfPage":{"@type":"WebPage","@id":"https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html"},"@type":"BlogPosting","url":"https://datapsyche.github.io/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html","dateModified":"2019-01-11T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://datapsyche.github.io/fastpages/feed.xml" title="False Positive" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastpages/">False Positive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastpages/about/">About Me</a><a class="page-link" href="/fastpages/search/">Search</a><a class="page-link" href="/fastpages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Data Mining Map by Saed Sayad</h1><p class="page-description">Data Mining Map by Saed Sayad.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-01-11T00:00:00-06:00" itemprop="datePublished">
        Jan 11, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/fastpages/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#data-mining-map-by-saeed-sayad">Data Mining Map by Saeed Sayad</a>
<ul>
<li class="toc-entry toc-h5"><a href="#now-what-do-we-do-in-data-science-">Now what do we do in Data Science ??</a></li>
<li class="toc-entry toc-h2"><a href="#explaining-the-past">Explaining the past</a></li>
<li class="toc-entry toc-h2"><a href="#predicting-the-future">Predicting the Future</a>
<ul>
<li class="toc-entry toc-h3"><a href="#classification-algorithms">Classification Algorithms</a></li>
</ul>
</li>
</ul>
</li>
</ul><h1 id="data-mining-map-by-saeed-sayad">
<a class="anchor" href="#data-mining-map-by-saeed-sayad" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Mining Map by Saeed Sayad</h1>

<p><strong>Data Science</strong> can be broadly divided into two approaches <em>explaining the past</em>  or <em>predicting the future</em> by means of data analysis. Data Science is a multi disciplinary field that combines statistics, machine learning, artificial intelligence and database technology.</p>

<p><img src="/home/jithin/github/datapsyche.github.io/img/2019-11-11-datascienceMap.png" alt=""></p>

<p>Business have accumulated data over the years and with the help of data science we are able extract valuable knowledge from this data. Lets understand how each field in above diagram contribute to Data Science.</p>

<p><strong>Statistics</strong> is used in Data Science for collecting , classifying, summarising, organising, analysing and interpreting data. <strong>Artificial Intelligence</strong> contributes to Data Science by simulating intelligent behaviours from the underlying data. <strong>Machine Learning</strong> contributes to data science by coming up with algorithms that improve automatically through experience. <strong>Database Technology</strong> is necessary for collecting, storing and managing data so users can retrieve, add, update or remove data.</p>

<h5 id="now-what-do-we-do-in-data-science-">
<a class="anchor" href="#now-what-do-we-do-in-data-science-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Now what do we do in Data Science ??</h5>

<p>In Laymans language we analyse data to <strong>explain the past</strong> or to <strong>predict the future</strong>.</p>

<h2 id="explaining-the-past">
<a class="anchor" href="#explaining-the-past" aria-hidden="true"><span class="octicon octicon-link"></span></a>Explaining the past</h2>

<p>For explaining the past we need to do Data Exploration - It is all about describing the data by means of statistical and visualization technique. Data Exploration helps in order to bring important aspects of data into focus for further analysis.</p>

<ul>
  <li>
    <p>Univariate Analysis -  Exploring variables one by one . Variables can be of two types categorical or numerical. Each type of variable has is own recommended way for analysis or for graphical plotting.</p>

    <ul>
      <li>
        <p>Categorical Variables - A categorical or discrete variable has two or more categories (values). There are two types of categorical variables</p>

        <ul>
          <li>Nominal Variable -  No intrinsic ordering for its categories (eg - Gender)</li>
          <li>Ordinal Variable - A clear ordering is there (eg - Temperature [low, medium, high]</li>
        </ul>

        <p><strong>Frequency tables</strong> are the best way to analyse such variables</p>

        <p><strong>Pie Chart</strong> and <strong>Bar Chart</strong> are commonly used for visual analysis.</p>

        <p><img src="/home/jithin/github/datapsyche.github.io/img/Univariate.png" alt=""></p>
      </li>
      <li>
        <p><strong>Numerical Variables</strong> - takes any value within a finite  or infinite interval eg:- <em>height, weight, temperature, blood glucose</em>. There are two types of numerical variables, intervals and ratio.</p>

        <ul>
          <li>
<strong>Interval Variables</strong> - Values whose differences are interpret able. (<em>temperature in centigrade</em>). Data in Interval scale can be added or subtracted but cannot be meaningfully multiplied or divided.</li>
          <li>
<strong>Ratio Variable</strong> - Data in ratio variable has values with a true zero and can be added, subtracted ,multiplied or divided. (eg- <em>weight</em>)</li>
        </ul>

        <p>How do we analyse Numerical variables ? Below table describe the various methods to analyse Numerical variable</p>

        <table>
          <thead>
            <tr>
              <th>Statistics</th>
              <th>Visualization</th>
              <th>Equation</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Count</td>
              <td>Histogram</td>
              <td><em>N</em></td>
              <td>Number of observations</td>
            </tr>
            <tr>
              <td>Minimum</td>
              <td>Box Plot</td>
              <td><em>Min</em></td>
              <td>smallest value among the observations</td>
            </tr>
            <tr>
              <td>Maximum</td>
              <td>Box Plot</td>
              <td><em>Max</em></td>
              <td>largest value among the observations</td>
            </tr>
            <tr>
              <td>Mean</td>
              <td>Box Plot</td>
              <td><img src="/home/jithin/github/datapsyche.github.io/img/mean.png" alt=""></td>
              <td>Sum of values / count</td>
            </tr>
            <tr>
              <td>Median</td>
              <td>Box Plot</td>
              <td><img src="/home/jithin/github/datapsyche.github.io/img/Median.png" alt=""></td>
              <td>Middle value below and above lies equal number of values</td>
            </tr>
            <tr>
              <td>Mode</td>
              <td>Histogram</td>
              <td> </td>
              <td>Most frequent value in observation set</td>
            </tr>
            <tr>
              <td>Quantile</td>
              <td>Box Plot</td>
              <td><em>Q~k~</em></td>
              <td>Cut points that divides observations into multiple groups with equal number of values</td>
            </tr>
            <tr>
              <td>Range</td>
              <td>Box plot</td>
              <td><em>Max - Min</em></td>
              <td>Difference between Maximum and minimum</td>
            </tr>
            <tr>
              <td>Variance</td>
              <td>Histogram</td>
              <td><img src="/home/jithin/github/datapsyche.github.io/img/Variance.png" alt=""></td>
              <td>Measure of Data Dispersion</td>
            </tr>
            <tr>
              <td>Standard Deviation</td>
              <td>Histogram</td>
              <td><img src="/home/jithin/github/datapsyche.github.io/img/StDev.png" alt=""></td>
              <td>Square root of Variance</td>
            </tr>
            <tr>
              <td>Coefficient Of Deviation</td>
              <td>Histogram</td>
              <td><img src="/home/jithin/github/datapsyche.github.io/img/CV.png" alt=""></td>
              <td>Measure of Data Dispersion divided by Mean</td>
            </tr>
            <tr>
              <td>Skewness</td>
              <td>Histogram</td>
              <td><img src="/home/jithin/github/datapsyche.github.io/img/Skewness.png" alt=""></td>
              <td>Measure of symmetry or asymmetry of data</td>
            </tr>
            <tr>
              <td>Kurtosis</td>
              <td>Histogram</td>
              <td> </td>
              <td>Measure of whether the data are peaked or flat relative to a normal distribution</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Bivariate Analysis</strong> - Simultaneous analysis of 2 Variables. Explores the concept of relationship between 2 variables. There are 3 types of bivariate analysis</p>

    <ul>
      <li>
<strong>Numerical &amp; Numerical</strong> -
        <ul>
          <li>Scatter Plot is a visual representation of two numerical variables, We can infer patterns from this.</li>
          <li>Linear Correlation - quantifies the linear relationship between two numerical variables</li>
        </ul>
      </li>
      <li>
<strong>Categorical &amp; Categorical</strong> -
        <ul>
          <li>Stacked Column Chart - Compares the percentage each category from one variable contributes to a total across categories of second variable</li>
          <li>Combination Charts - two or more different type of charts for each variable (bar chart and chart) to show how one variable is affecting other variable</li>
          <li>Chi Square Test -Used to determine association between categorical variables, based on differences between expected frequencies (e) and observed frequency (n) in one or more categories in the frequency  table. The test returns a probability for the computed chi square and degree of freedom, probability of 0 means complete dependency between categorical variable and probability of 1 means two categorical variables are completely independent</li>
        </ul>
      </li>
      <li>
<strong>Numerical &amp; Categorical</strong> -
        <ul>
          <li>Line Chart with error Bars -Error Bars show standard Error in that particular Category.</li>
          <li>Combination Chart - either line or Bar chart ( line for numerical variable and bar for categorical)</li>
          <li>Z test and T test - Assess averages of two groups are statistically different from each other If probability between Z is small the difference between two averages is more significant. We use T test when number of observation is less than 30</li>
          <li>Anova test Assess whether the averages of more than two groups are statistically different from each other, Analysis is appropriate for comparing the averages of a numerical variable for more than two categories of a categorical variable.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="predicting-the-future">
<a class="anchor" href="#predicting-the-future" aria-hidden="true"><span class="octicon octicon-link"></span></a>Predicting the Future</h2>

<p>For predicting the future we make use of models. Hence the name Predictive Modeling. Here we try to predict the outcome. if the outcome is Categorical we call it <strong>classification</strong>, if it is numerical we call it <strong>regression</strong>. Descriptive modelling or <strong>clustering</strong> is the assignment of observations into clusters. <strong>Association Rule</strong> can help us find interesting association among observations.</p>

<h3 id="classification-algorithms">
<a class="anchor" href="#classification-algorithms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification Algorithms</h3>

<p>Here output variable is categorical. Classification algorithms could be broadly divided into 4 main groups.</p>

<ul>
  <li>
    <p>Frequency Table Based</p>

    <ul>
      <li>
        <p>Zero R method - simplest classification method exclusively relies on target and ignores all predictors. ZeroR classifier simply predicts the majority class. It has no predictability power however it is usefull for determining Baseline performance as a benchmark for other classification methods.</p>

        <p><code class="highlighter-rouge">Construct a frequency  table and select its most frequent value.</code></p>
      </li>
      <li>One R method - simple yet accurate classifiaction algorithm that generates one rule for each predictor in the data and then selects the rule with smallest total error as its one rule.
        <pre><code class="language-General">For each predictor,
        For each value of that predictor make a rule as follows:
                Count how often each value of target appears.
                Find the most frequent class
                Make the rule assign that class to this value of the predictor
        Calculate the total error of the rules of each predictor
Choose the predictor with smallest total error
</code></pre>
      </li>
      <li>
        <p>Naive Bayes Method -  Based on Naive Bayes Theorem with independence assumption between predictors. Naive Bayes model is easy to build, useful for large data set. Naive Bayes often useful and is widely used as it outperforms classification methods.
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">/</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">/</mi><mi>c</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(c/x) = (P(x/c)*P(c))/P(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">/</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord">/</span><span class="mord mathdefault">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">/</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(c/x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">/</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> - Posterior Probability</p>

        <p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">/</mi><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x/c)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord">/</span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span> - Likelihood</p>

        <p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(c)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span> - Class probability</p>

        <p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> - Predictor Prior Probability</p>

        <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">/</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi mathvariant="normal">/</mi><mi>C</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mtext> </mtext><mn>2</mn><mtext> </mtext><mi mathvariant="normal">/</mi><mi>C</mi><mo stretchy="false">)</mo><mi>x</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mtext> </mtext><mi>n</mi><mtext> </mtext><mi mathvariant="normal">/</mi><mi>C</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>P</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(c/X)  = P(x~1~/C) * P(x~2~/C) x ....P(x~n~/C) * P(C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace nobreak"> </span><span class="mord">1</span><span class="mspace nobreak"> </span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace nobreak"> </span><span class="mord">2</span><span class="mspace nobreak"> </span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mord mathdefault">x</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace nobreak"> </span><span class="mord mathdefault">n</span><span class="mspace nobreak"> </span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span></span>
      </li>
    </ul>
  </li>
</ul>


  </div><a class="u-url" href="/fastpages/markdown/2019/01/11/Machine-Learning-Refresher.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastpages/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastpages/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastpages/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blog to track progress in data science.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/fastpages/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/fastpages/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
